# Reading 2 (Data Bias)

### Material
##### Listen
- [Virginia Eubanks, Automating Inequality with ](https://www.writersvoice.net/tag/virginia-eubanks/) by Writer's Voice Podcast (2019)
- [AI: The Problem with Bias, with Kate Crawford](https://open.spotify.com/episode/0ysGO67iXaPmTx4h9v33z3?si=FmJeEuyJTeiqckjpHCTlVQ), City Arts & Lectures Podcast (2018)

##### Watch
- [Machine Learning and Human Bias](https://www.youtube.com/watch?v=59bMh59JQDo), Video by Google (2017)
- [How I'm fighting bias in algorithms](https://www.youtube.com/watch?v=UG_X_7g63rY), Ted Talk by Joy Buolamwini (2017)
- [Invisible Images Of Surveillance](https://www.youtube.com/watch?v=ijVTdSoZEC4), Trevor Paglen (2018)

### Prompts
>How the technical tools promise to *fair out* the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?

>Imagine, what could this (following quote) mean in the widest sense?
"*The state doesn't need a cop to kill a person" and "electronic incarceration*"

>What do you understand this to mean?
"*systems act as a kind of 'empathy-overwrite'*"

>China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges ([recent example](https://www.nytimes.com/2020/03/01/business/china-coronavirus-surveillance.html?)). Try to point example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of
"*technical systems not well thought-through about what their impact on human beings is*"

### Response
First of all, I think technical tools expand the spectrum of information that could be loaded in the public system. Compared to social workers manually recording case by case, machines could enter data of larger populations into the system. So in this sense, technical tools lead to an improvement in the working efficiency of the social/welfare system. For example, in China, a huge database system is created to record citizens in poverty, so the governments (both local and central) could access their information, target and help those living in hardship. It would be a mission impossible without the powerful database management system. However, it is also obvious that there would not be as much in-depth connections and humane conversation involved in the wide-scale inclusion of database systems and machine learning algorithms.

In today's readings, all of the materials seem to point to the direction that human intervention and auditing is still needed in the fields where human workers are gradually replaced by AI. In addition, the materials have also helped me develop a perspective on human bias and its perpetuation in technology. Since computer programs are coded by humans, and usually just a few "genius" mechanics. So even with good intentions, it is nearly impossible to separate ourselves from our own human biases. Thus, our human biases become part of the technology we create. So if an individual is bound to be biased by their surroundings, views, experiences, etc., will there be any solutions to data bias? And what are the differences in human bias and data/machine bias?  If combining both human and computing forces, is it gonna improve or worsen the situations of those impacted or neglected by the algorithmic world? Some of these questions are answered, while others are not.

Before learning about the stories and societal context in *Automating Inequality*, I would 100 percent think of the introduction of machine-based diagnosis in social/welfare systems as a total blessing, or at least good enough given what we already have. But I have this biased opinion because I have never been targeted by the system. The moral diagnosis of the system is not adequate to provide answers for every household. And a little error caused by the system could lead great harm to the lives of a person and an entire family.

However, with the optimism shown in Virginia Eubanks's ending remark, I do not think now is a bad starting point either. Identifying bias, being aware of the social impacts of the technology that we're developing and collecting wider, more inclusive datasets would break down the barriers raised by malfunctioning algorithms. The magic of technology should not be a privilege, but a human right.
